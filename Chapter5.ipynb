{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation (RAG) \n",
    "\n",
    "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\n",
    "\n",
    "\n",
    "### Q & A  with RAG\n",
    "https://python.langchain.com/docs/use_cases/question_answering/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical RAG application has two main components:\n",
    "\n",
    "- Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n",
    "- Retrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most common full sequence from raw data to answer looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"# LangChain\\nComplete understanding of Open AI LangChain \\n\\nChapters:\\n1. LangChain integaration and understanding prompt, chaining and history collecting with ConversationBufferMemory\\n2. Further understanding of prompts and FewShotPromptTemplate\\n3. PDF Query Using Langchain Q/A chain (Includes Text Embedding for classification, Search, recommendation, detection, etc)\\n4. LLM Model with Deployment using HuggingFace Spaces\\n5. RAG (Retrieval Augmentated Generation)\\n\\n\\n## Working with langChain\\n- Create your openAI API key \\n- Generate HuggingFace access token (Read)\\n- Install all the requirements \\n- Run the required chapter python file using streamlit run\\n\\n## Streamlit Run\\n\\n- run on local host: http://localhost:8501/\\n\\n## Deploy to HuggingFace spaces\\n\\n- Create new Space with SDK paramenter 'Streamlit':\\nhttps://huggingface.co/spaces/rasika-gulhane/LLM_Chatbot_Q-A\\n\\n- Follow the steps to deploy git repo\\n- Spaces > Settings : Add the secrets for OpenAI access key\\n- Go to App in Home page\\n- Good to Run\\n\\n\", metadata={'source': 'README.md'})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load text files \n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"README.md\")\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. SPLIT \n",
    "# Split: Text splitters break large Documents into smaller chunks.\n",
    "# This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won’t fit in a model’s finite context window.\n",
    "\n",
    "\n",
    "# - Split the text up into small, semantically meaningful chunks (often sentences).\n",
    "# - Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\n",
    "# - Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Embedding and Vector store:\n",
    "\n",
    "# - Store: We need somewhere to store and index our splits, so that they can later be searched over. \n",
    "# - This is often done using a VectorStore and Embeddings model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval and generation\n",
    "- Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
    "- Generate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Retrival in LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_openai\n",
      "  Using cached langchain_openai-0.1.3-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in ./LCenv/lib/python3.9/site-packages (from langchain_openai) (0.1.42)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in ./LCenv/lib/python3.9/site-packages (from langchain_openai) (1.18.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in ./LCenv/lib/python3.9/site-packages (from langchain_openai) (0.6.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./LCenv/lib/python3.9/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain_openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./LCenv/lib/python3.9/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain_openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in ./LCenv/lib/python3.9/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain_openai) (0.1.47)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in ./LCenv/lib/python3.9/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain_openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./LCenv/lib/python3.9/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain_openai) (2.7.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./LCenv/lib/python3.9/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain_openai) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./LCenv/lib/python3.9/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./LCenv/lib/python3.9/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./LCenv/lib/python3.9/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in ./LCenv/lib/python3.9/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./LCenv/lib/python3.9/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./LCenv/lib/python3.9/site-packages (from openai<2.0.0,>=1.10.0->langchain_openai) (4.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./LCenv/lib/python3.9/site-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2024.4.16)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./LCenv/lib/python3.9/site-packages (from tiktoken<1,>=0.5.2->langchain_openai) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./LCenv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain_openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./LCenv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain_openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in ./LCenv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in ./LCenv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./LCenv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain_openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./LCenv/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.42->langchain_openai) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./LCenv/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.42->langchain_openai) (3.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./LCenv/lib/python3.9/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.42->langchain_openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in ./LCenv/lib/python3.9/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.42->langchain_openai) (2.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./LCenv/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain_openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./LCenv/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain_openai) (2.2.1)\n",
      "Using cached langchain_openai-0.1.3-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: langchain_openai\n",
      "Successfully installed langchain_openai-0.1.3\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = some_retrival_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "chain = (\n",
    "    \n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(\"What did the president say about technology?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Retriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s implement a toy retriever that returns all documents whose text contains the text in the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyRetriever(BaseRetriever):\n",
    "\n",
    "    \"\"\"A toy retriever that contains the top k documents that contain the user query.\n",
    "\n",
    "    This retriever only implements the sync method _get_relevant_documents.\n",
    "\n",
    "    If the retriever were to involve file access or network access, it could benefit\n",
    "    from a native async implementation of `_aget_relevant_documents`.\n",
    "\n",
    "    As usual, with Runnables, there's a default async implementation that's provided\n",
    "    that delegates to the sync implementation running on another thread.\n",
    "    \"\"\"\n",
    "\n",
    "    documents: List[Document]\n",
    "    \"\"\"List of documents to retrieve from.\"\"\"\n",
    "\n",
    "    k: int\n",
    "    \n",
    "    \"\"\"Number of top results to return\"\"\"\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        \n",
    "        \"\"\"Sync implementations for retriever.\"\"\"\n",
    "        matching_documents = []\n",
    "        for document in self.documents:\n",
    "            if len(matching_documents) > self.k:\n",
    "                return matching_documents\n",
    "\n",
    "            if query.lower() in document.page_content.lower():\n",
    "                matching_documents.append(document)\n",
    "        return matching_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test this:\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
    "        metadata={\"type\": \"dog\", \"trait\": \"loyalty\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
    "        metadata={\"type\": \"cat\", \"trait\": \"independence\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Goldfish are popular pets for beginners, requiring relatively simple care.\",\n",
    "        metadata={\"type\": \"fish\", \"trait\": \"low maintenance\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Parrots are intelligent birds capable of mimicking human speech.\",\n",
    "        metadata={\"type\": \"bird\", \"trait\": \"intelligence\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Rabbits are social animals that need plenty of space to hop around.\",\n",
    "        metadata={\"type\": \"rabbit\", \"trait\": \"social\"},\n",
    "    ),\n",
    "]\n",
    "retriever = ToyRetriever(documents=documents, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add unit or integration tests to verify that invoke and ainvoke work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Cats are independent pets that often enjoy their own space.', metadata={'type': 'cat', 'trait': 'independence'}),\n",
       " Document(page_content='Rabbits are social animals that need plenty of space to hop around.', metadata={'type': 'rabbit', 'trait': 'social'})]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"that\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Cats are independent pets that often enjoy their own space.', metadata={'type': 'cat', 'trait': 'independence'}),\n",
       " Document(page_content='Rabbits are social animals that need plenty of space to hop around.', metadata={'type': 'rabbit', 'trait': 'social'})]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await retriever.ainvoke(\"that\")\n",
    "\n",
    "# It’s a runnable so it’ll benefit from the standard Runnable Interface! 🤩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(page_content='Dogs are great companions, known for their loyalty and friendliness.', metadata={'type': 'dog', 'trait': 'loyalty'})],\n",
       " [Document(page_content='Cats are independent pets that often enjoy their own space.', metadata={'type': 'cat', 'trait': 'independence'})]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.batch([\"dog\", \"cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_retriever_start', 'run_id': '110b0196-169c-4208-aadd-f1cfb32cfe7b', 'name': 'ToyRetriever', 'tags': [], 'metadata': {}, 'data': {'input': 'bar'}}\n",
      "{'event': 'on_retriever_stream', 'run_id': '110b0196-169c-4208-aadd-f1cfb32cfe7b', 'tags': [], 'metadata': {}, 'name': 'ToyRetriever', 'data': {'chunk': []}}\n",
      "{'event': 'on_retriever_end', 'name': 'ToyRetriever', 'run_id': '110b0196-169c-4208-aadd-f1cfb32cfe7b', 'tags': [], 'metadata': {}, 'data': {'output': []}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rasikagulhane/Desktop/LangChain/LCenv/lib/python3.9/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: This API is in beta and may change in the future.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "async for event in retriever.astream_events(\"bar\", version=\"v1\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
